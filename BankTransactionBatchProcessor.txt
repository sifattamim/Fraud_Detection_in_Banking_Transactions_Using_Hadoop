Batch Layer Solution Steps

Step 1: Data Ingestion using Sqoop
Imported card_member and member_score tables from AWS RDS into HDFS:
sqoop import --connect jdbc:mysql://<amazon-rds>:3306/cred_financials_data \
--username <username> --password <password> \
--table card_member \
--warehouse-dir /user/ec2-user/CCFraudDetection/input/awsrdstables

sqoop import --connect jdbc:mysql://<amazon-rds>:3306/cred_financials_data \
--username <username> --password <password> \
--table member_score \
--warehouse-dir /user/ec2-user/CCFraudDetection/input/awsrdstables


Step 2: Move Local CSV to HDFS
hadoop fs -mkdir -p /user/ec2-user/CCFraudDetection/input
hadoop fs -put card_transactions.csv /user/ec2-user/CCFraudDetection/input/


Step 3: Load Data into Hive & HBase
3.1. Create Hive table for transaction history:
CREATE EXTERNAL TABLE IF NOT EXISTS card_transactions_history_data (
  card_id BIGINT, member_id BIGINT, amount DOUBLE, postcode INT,
  pos_id BIGINT, transaction_dt STRING, status STRING
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/user/ec2-user/CCFraudDetection/input/hive/'
TBLPROPERTIES ("skip.header.line.count"="1");

LOAD DATA INPATH '/user/ec2-user/CCFraudDetection/input/card_transactions.csv'
OVERWRITE INTO TABLE card_transactions_history_data;
3.2. Create HBase table via Hive:
create 'card_transactions_master', 'cardDetail', 'transactionDetail'
CREATE EXTERNAL TABLE IF NOT EXISTS card_transactions_hbase_master (
  rowid STRING, card_id BIGINT, member_id BIGINT, amount DOUBLE,
  postcode INT, pos_id BIGINT, transaction_dt STRING, status STRING
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
  "hbase.columns.mapping" = ":key, cardDetail:card_id, cardDetail:member_id,
   transactionDetail:amount, transactionDetail:postcode, transactionDetail:pos_id,
   transactionDetail:transaction_dt, transactionDetail:status"
)
TBLPROPERTIES ("hbase.table.name" = "card_transactions_master");

INSERT OVERWRITE TABLE card_transactions_hbase_master
SELECT regexp_replace(reflect("java.util.UUID","randomUUID"), "-", "") AS rowid,
       card_id, member_id, amount, postcode, pos_id, transaction_dt, status
FROM card_transactions_history_data;


Step 4: Create Lookup Table (HBase via Hive)
create 'card_transaction_lookup', 'cf'
CREATE EXTERNAL TABLE IF NOT EXISTS card_transactions_lookup (
  card_id BIGINT, ucl DOUBLE, postcode INT, transaction_dt STRING, score INT
)
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
  "hbase.columns.mapping" = ":key, cf:ucl, cf:postcode, cf:transaction_dt, cf:score"
)
TBLPROPERTIES ("hbase.table.name" = "card_transaction_lookup");


Step 5: Prepare Hive Tables for Batch Computation
-- card_member table
CREATE EXTERNAL TABLE IF NOT EXISTS card_member (
  card_id BIGINT, member_id BIGINT, member_joining_dt STRING,
  card_purchase_dt STRING, country STRING, city STRING
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/user/ec2-user/CCFraudDetection/input/hive/card_member/';

-- member_score table
CREATE EXTERNAL TABLE IF NOT EXISTS member_score (
  member_id BIGINT, score INT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/user/ec2-user/CCFraudDetection/input/hive/member_score/';


Step 6: Create Staging Tables (ORC format)
-- Score per card
CREATE EXTERNAL TABLE IF NOT EXISTS card_score (
  card_id BIGINT, score INT
)
STORED AS ORC LOCATION '/user/ec2-user/CCFraudDetection/input/hive/card_score/'
TBLPROPERTIES ("orc.compress" = "SNAPPY");

-- Last 10 transactions
CREATE EXTERNAL TABLE IF NOT EXISTS card_last_ten_transactions (
  card_id BIGINT, amount DOUBLE, postcode INT, transaction_dt STRING, status STRING
)
STORED AS ORC LOCATION '/user/ec2-user/CCFraudDetection/input/hive/card_last_ten_transactions/'
TBLPROPERTIES ("orc.compress" = "SNAPPY");

-- UCL values
CREATE EXTERNAL TABLE IF NOT EXISTS card_ucl (
  card_id BIGINT, ucl DOUBLE
)
STORED AS ORC LOCATION '/user/ec2-user/CCFraudDetection/input/hive/card_ucl/'
TBLPROPERTIES ("orc.compress" = "SNAPPY");

-- Latest zipcode & transaction date
CREATE EXTERNAL TABLE IF NOT EXISTS card_zipcode (
  card_id BIGINT, postcode INT, transaction_dt STRING
)
STORED AS ORC LOCATION '/user/ec2-user/CCFraudDetection/input/hive/card_zipcode/'
TBLPROPERTIES ("orc.compress" = "SNAPPY");


Step 7: Load Ingested Data into Hive
LOAD DATA INPATH '/user/ec2-user/CCFraudDetection/input/awsrdstables/card_member/part*'
OVERWRITE INTO TABLE card_member;

LOAD DATA INPATH '/user/ec2-user/CCFraudDetection/input/awsrdstables/member_score/part*'
OVERWRITE INTO TABLE member_score;


Step 8: Join Member Score with Cards
INSERT OVERWRITE TABLE card_score
SELECT cm.card_id, ms.score
FROM card_member cm
JOIN member_score ms ON cm.member_id = ms.member_id;


Step 9: Get Last 10 Genuine Transactions
INSERT OVERWRITE TABLE card_last_ten_transactions
SELECT card_id, amount, postcode, transaction_dt, status
FROM (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY card_id ORDER BY unix_timestamp(transaction_dt,'yyyy-MM-dd HH:mm:ss') DESC) AS rn
  FROM card_transactions_hbase_master
  WHERE status = 'GENUINE'
) t
WHERE t.rn <= 10;

Step 10: Calculate UCL (Upper Control Limit)
INSERT OVERWRITE TABLE card_ucl
SELECT card_id, (AVG(amount) + 3 * STDDEV(amount)) AS ucl
FROM card_last_ten_transactions
GROUP BY card_id;


Step 11: Get Latest Zipcode and Transaction Date
INSERT OVERWRITE TABLE card_zipcode
SELECT card_id, postcode, transaction_dt
FROM (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY card_id ORDER BY unix_timestamp(transaction_dt,'yyyy-MM-dd HH:mm:ss') DESC) AS rn
  FROM card_last_ten_transactions
) t
WHERE t.rn = 1;


Step 12: Final Join to Create Lookup Table
INSERT OVERWRITE TABLE card_transactions_lookup
SELECT cs.card_id, cu.ucl, cz.postcode, cz.transaction_dt, cs.score
FROM card_score cs
JOIN card_ucl cu ON cs.card_id = cu.card_id
JOIN card_zipcode cz ON cs.card_id = cz.card_id;

